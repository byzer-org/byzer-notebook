package io.kyligence.notebook.console;

import io.kyligence.notebook.console.tools.ImportResponseDTO;
import io.kyligence.notebook.console.util.JacksonUtils;
import lombok.extern.slf4j.Slf4j;
import org.junit.Before;
import org.junit.ClassRule;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.mockserver.client.MockServerClient;
import org.mockserver.model.ClearType;
import org.mockserver.model.HttpRequest;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.boot.test.context.TestConfiguration;
import org.springframework.boot.test.web.client.TestRestTemplate;
import org.springframework.boot.web.server.LocalServerPort;
import org.springframework.http.ResponseEntity;
import org.springframework.mock.web.MockMultipartFile;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.test.context.TestPropertySource;
import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;
import org.springframework.test.web.servlet.MockMvc;
import org.springframework.test.web.servlet.ResultActions;
import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;
import org.springframework.web.context.WebApplicationContext;
import org.testcontainers.containers.MockServerContainer;
import org.testcontainers.containers.MySQLContainer;
import org.testcontainers.containers.Network;
import org.testcontainers.utility.DockerImageName;

import javax.annotation.PostConstruct;
import java.io.File;
import java.net.URL;
import java.util.Objects;

import static org.junit.Assert.assertTrue;
import static org.mockserver.model.HttpRequest.request;
import static org.mockserver.model.HttpResponse.response;

/**
 * 07/03/2022 hellozepp(lisheng.zhanglin@163.com)
 */
@Slf4j
@RunWith(SpringJUnit4ClassRunner.class)
@SpringBootTest(classes = {NotebookLauncher.class, RelationalDbDaoConfiguration.class}, webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)
@TestPropertySource(value = "classpath:notebook.properties")
@ActiveProfiles("test")
@AutoConfigureMockMvc
public class NotebookLauncherBaseTest extends BaseResourceLoader {
    private static final String etResponse = "[{\"name\":\"ALSInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n<a href=\\\"https://en.wikipedia.org/wiki/Matrix_completion\\\">Alternating Least Squares (ALS)</a>\\n\\nThe alternating least squares (ALS) algorithm factorizes a given matrix R into two factors U and\\nV such that R≈UTV. The unknown row dimension is given as a parameter to the algorithm and is\\ncalled latent factors. Since matrix factorization can be used in the context of recommendation,\\nthe matrices U and V can be called user and item matrix, respectively.\\n\\n Use \\\"load modelParams.`ALSInPlace` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\",\"docType\":\"html\"},{\"name\":\"Assert\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"AutoIncrementKeyExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"AutoML\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n AutoML is an extension for finding the best models among GBT, LinearRegression,LogisticRegression,\\n NaiveBayes and RandomForest classifiers.\\n\\n It only supports binary labels and sorted by custmoized performance key.\\n\\n Use \\\"load modelParams.`AutoML` as output;\\\"\\n\\n to check the available parameters;\\n\\n Use \\\"load modelExample.`AutoML` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"AutoML\\\" as outout;\\n ```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"CacheExt\",\"algType\":\"feature engineer\",\"sparkCompatibility\":\"\",\"doc\":\"\\nSQLCacheExt is used to cache/uncache table.\\n\\n```sql\\nrun table as CacheExt.`` where execute=\\\"cache\\\" and isEager=\\\"true\\\";\\n```\\n\\nIf you execute the upper command, then table will be cached immediately, othersise only the second time\\nto use the table you will fetch the table from cache.\\n\\nTo release the table , do like this:\\n\\n```sql\\nrun table as CacheExt.`` where execute=\\\"uncache\\\";\\n```\\n    \",\"docType\":\"md\"},{\"name\":\"CommunityBasedSimilarityInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ConfusionMatrix\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ConnectPersistCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n\\n```\\n\\nexample\\n    \\n```\\n    \",\"docType\":\"md\"},{\"name\":\"CopyFromLocal\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"CorpusExplainInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"DTF\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"DataSourceExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"DeltaCommandWrapper\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"DeltaCompactionCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"DicOrTableToArray\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Discretizer\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Discretization\\\">Discretization</a>\\nIn applied mathematics, discretization is the process of transferring continuous functions,\\nmodels, variables, and equations into discrete counterparts.\\nThis process is usually carried out as a first step toward making them suitable for numerical\\nevaluation and implementation on digital computers. Dichotomization is the special case of\\ndiscretization in which the number of discrete classes is 2,\\nwhich can approximate a continuous variable as a binary variable\\n(creating a dichotomy for modeling purposes, as in binary classification).\\n\\n Use \\\"load modelParams.`Discretizer` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\\n    \",\"docType\":\"html\"},{\"name\":\"DownloadExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ElifCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ElseCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"EmptyTable\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"EmptyTableWithSchema\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"EngineResource\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ExternalPythonAlg\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n\\nRequirements:\\n\\n1. Conda is installed in your cluster.\\n2. The user who runs StreamingPro cluster has the permission to read/write `/tmp/__mlsql__`.\\n\\nSuppose you run StreamingPro/MLSQL with user named `mlsql`.\\nConda should be installed by `mlsql` and `mlsql` have the permission to read/write `/tmp/__mlsql__`.\\n\\nYou can get code example by:\\n\\n```\\nload modelExample.`PythonAlg` as output;\\n```\\n\\nActually, this doc is also can be get by this command.\\n\\nIf you wanna know what params the PythonAlg have, please use the command following:\\n\\n```\\nload modelParam.`PythonAlg` as output;\\n```\\n\\n     \",\"docType\":\"md\"},{\"name\":\"FPGrowth\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"FeatureExtractInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"FeishuMessageExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n FeishuMessageExt sends specified text to a feishu webhook.\\n Please follow https://www.feishu.cn/hc/zh-CN/articles/360024984973 to get a webhook\\n Use \\\"load modelExample.`FeishuMessageExt` as output;\\\" to see the codeExample.\\n    \",\"docType\":\"md\"},{\"name\":\"FiCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"GBTClassifier\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Gradient_boosting\\\">Gradient Boosting</a> is a\\n machine learning technique for regression, classification and other tasks,\\n which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees\\n\\n Use \\\"load modelParams.`GBTClassifier` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`GBTClassifier` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"GBTClassifier\\\" as outout;\\n ```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"GBTRegressor\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Gradient_boosting\\\">Gradient Boosting</a> is a\\n machine learning technique for regression, classification and other tasks,\\n which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees\\n\\n Use \\\"load modelParams.`GBTRegressor` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`GBTRegressor` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"GBTRegressor\\\" as outout;\\n ```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"GBTs\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"HDFSCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"HashTfIdf\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"IfCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"IteratorCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"JDBC\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"JDBCUpdatExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"JsonExpandExt\",\"algType\":\"feature engineer\",\"sparkCompatibility\":\"\",\"doc\":\"\\n JsonExpandExt is used to expand json strings, please\\n see the codeExample to learn its usage.\\n\\n Use \\\"load modelExample.`JsonExpandExt` as output;\\\"\\n to see the codeExample.\\n    \",\"docType\":\"md\"},{\"name\":\"KMeans\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n<a href=\\\"https://en.wikipedia.org/wiki/K-means_clustering\\\"> k-means clustering </a>\\n\\nk-means clustering is a method of vector quantization, originally from signal processing,\\nthat aims to partition n observations into k clusters in which each observation belongs to\\nthe cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype\\nof the cluster. This results in a partitioning of the data space into Voronoi cells.\\nk-means clustering minimizes within-cluster variances (squared Euclidean distances),\\nbut not regular Euclidean distances, which would be the more difficult Weber problem:\\nthe mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances.\\nFor instance, better Euclidean solutions can be found using k-medians and k-medoids.\\n\\n Use \\\"load modelParams.`KMeans` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\",\"docType\":\"html\"},{\"name\":\"KafkaCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Kill\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"LDA\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"http://en.wikipedia.org/wiki/LDA\\\">LDA</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n1. To load data\\n```\\nload libsvm.`D:/soucecode/spark-2.3-src/data/mllib/sample_lda_libsvm_data.txt` as data1;\\n```\\n\\n2. To train LDA Model\\n```\\ntrain data1 as LDA.`/tmp/model` where\\n```\\n\\n-- k: number of topics, or number of clustering centers\\n```\\nk=\\\"3\\\"\\n```\\n\\n-- docConcentration: the hyperparameter (Dirichlet distribution parameter) of article distribution must be >1.0. The larger the value is, the smoother the predicted distribution is\\n```\\nand docConcentration=\\\"3.0\\\"\\n```\\n\\n-- topictemperature: the hyperparameter (Dirichlet distribution parameter) of the theme distribution must be >1.0. The larger the value is, the more smooth the distribution can be inferred\\n```\\nand topicConcentration=\\\"3.0\\\"\\n```\\n\\n-- maxIterations: number of iterations, which need to be fully iterated, at least 20 times or more\\n```\\nand maxIter=\\\"100\\\"\\n```\\n\\n-- setSeed: random seed\\n```\\nand seed=\\\"10\\\"\\n```\\n\\n-- checkpointInterval: interval of checkpoints during iteration calculation\\n```\\nand checkpointInterval=\\\"10\\\"\\n```\\n\\n-- optimizer: optimized calculation method currently supports \\\"em\\\" and \\\"online\\\". Em method takes up more memory, and multiple iterations of memory may not be enough to throw a stack exception\\n```\\nand optimizer=\\\"online\\\";\\n```\\n\\n3. register LDA to UDF\\n```\\nregister LDA.`C:/tmp/model` as lda;\\n```\\n\\n4. use LDA udf\\n```\\nselect label,lda(4) topicsMatrix,lda_doc(features) TopicDistribution,lda_topic(label,4) describeTopics from data as result;\\n```\\n\\n5. save result\\n```\\nsave overwrite result as json.`/tmp/result`;\\n```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"LSVM\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"LastCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\nWhen you want to get the result from command and used\\n in next command(SQL), you can use !last command.\\n\\nFor example:\\n\\n```\\n\\n!hdfs /tmp;\\n!last named hdfsTmpTable;\\nselect * from hdfsTmpTable;\\n    \\n```\\n    \",\"docType\":\"md\"},{\"name\":\"LastTableName\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"LinearRegression\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Linear_Regression\\\">Linear Regression</a> learning algorithm for\\n classification.\\n It usually used for prediction/forecasting/error reduction and variation explain.\\n\\n Use \\\"load modelParams.`LinearRegression` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`LinearRegression` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"LinearRegression\\\" as outout;\\n ```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"LogisticRegression\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Logistic_regression\\\">Logistic Regression</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n  Outputs with more than two values are modeled by multinomial logistic regression and,\\n  if the multiple categories are ordered, by ordinal logistic regression\\n  (for example the proportional odds ordinal logistic mode）\\n\\n Use \\\"load modelParams.`LogisticRegression` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`LogisticRegression` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"LogisticRegression\\\" as outout;\\n ```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"MLSQLEventCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"MLSQLWatcherCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Map\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"MapValues\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ModelCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ModelExplainInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"NaiveBayes\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\\\">Naive Bayes</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`NaiveBayes` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`NaiveBayes` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"NaiveBayes\\\" as output;\\n ```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"NormalizeInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Normalization_(statistics)\\\">Normalization</a>\\n\\n In statistics and applications of statistics,\\n normalization can have a range of meanings.\\n In the simplest cases, normalization of ratings means adjusting values measured on different scales to a\\n notionally common scale, often prior to averaging. In more complicated cases, normalization may refer to\\n more sophisticated adjustments where the intention is to bring the entire probability distributions of adjusted\\n values into alignment. In the case of normalization of scores in educational assessment, there may be an\\n intention to align distributions to a normal distribution.\\n A different approach to normalization of probability distributions is quantile normalization, where the\\n quantiles of the different measures are brought into alignment.\\n\\n Use \\\"load modelParams.`NormalizeInPlace` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\\n    \",\"docType\":\"html\"},{\"name\":\"NothingET\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"PageRank\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Pivot\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"PluginCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"PredictionEva\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers\\\"> PredictionEva </a> is an extension for evaluating the prediction result.\\n\\n It will evaluate the prediction results from f1-score, precision, recall, auc value and prc value.\\n\\n    \",\"docType\":\"html\"},{\"name\":\"PrintCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ProfilerCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"PythonAlg\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n\\nRequirements:\\n\\n1. Conda is installed in your cluster.\\n2. The user who runs StreamingPro cluster has the permission to read/write `/tmp/__mlsql__`.\\n\\nSuppose you run StreamingPro/MLSQL with user named `mlsql`.\\nConda should be installed by `mlsql` and `mlsql` have the permission to read/write `/tmp/__mlsql__`.\\n\\nYou can get code example by:\\n\\n```\\nload modelExample.`PythonAlg` as output;\\n```\\n\\nActually, this doc is also can be get by this command.\\n\\nIf you wanna know what params the PythonAlg have, please use the command following:\\n\\n```\\nload modelParam.`PythonAlg` as output;\\n```\\n\\n     \",\"docType\":\"md\"},{\"name\":\"PythonAlgBP\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"PythonCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"PythonEnvExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"PythonInclude\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n\\n```\\n\\nexample\\n    \\n```\\n    \",\"docType\":\"md\"},{\"name\":\"PythonParallelExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"RandomForest\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"http://en.wikipedia.org/wiki/Random_forest\\\">Random Forest</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`RandomForest` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`RandomForest` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"RandomForest\\\" as outout;\\n ```\\n\\n    \",\"docType\":\"html\"},{\"name\":\"RateSampler\",\"algType\":\"feature engineer\",\"sparkCompatibility\":\"\",\"doc\":\"\\n Splits dataset into train and test, splitting ratio is specified\\n by parameter sampleRate.\\n    \",\"docType\":\"md\"},{\"name\":\"RawSimilarInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Ray\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\" \",\"docType\":\"md\"},{\"name\":\"ReduceFeaturesInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"RepartitionExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"RowMatrix\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"RunScript\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\nWhen you want to get the result from command and used\\n in next command(SQL), you can use !last command.\\n\\nFor example:\\n\\n```\\n\\n!runScript ''' select 1 as a as b; ''' named output;\\n    \\n```\\n    \",\"docType\":\"md\"},{\"name\":\"SaveBinaryAsFile\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ScalaScriptUDF\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n## Script support\\n\\nScript e.g. Python,Scala nested in MLSQL provides more fine-grained control when doing some ETL tasks, as it allows you\\neasily create SQL function with more powerful language which can do complex logical task.\\n\\nCause the tedious of java's grammar, we will not support java script.\\n\\nBefore use ScriptUDF module, you can use\\n\\n```\\nload modelParams.`ScriptUDF` as output;\\n```\\n\\nto check how to configure this module.\\n\\n### Python UDF Script Example\\n\\n```sql\\n-- using set statement to hold your python script\\n-- Notice that the first parameter of function you defined should be self.\\nset echoFun='''\\n\\ndef apply(self,m):\\n    return m\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be processed more conveniently.\\nload script.`echoFun` as scriptTable;\\n\\n-- register `apply` as UDF named `echoFun`\\nregister ScriptUDF.`scriptTable` as echoFun options\\n-- specify which script you choose\\nand lang=\\\"python\\\"\\n-- As we know python is not strongly typed language, so\\n-- we should manually spcify the return type.\\n-- map(string,string) means a map with key is string type,value also is string type.\\n-- array(string) means a array with string type element.\\n-- nested is support e.g. array(array(map(string,array(string))))\\nand dataType=\\\"map(string,string)\\\"\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect echoFun(map('a','b')) as res from dataTable as output;\\n```\\n\\n### Scala UDF Script Example\\n\\n```sql\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be process more convenient.\\nload script.`plusFun` as scriptTable;\\n\\n-- register `apply` as UDF named `plusFun`\\nregister ScriptUDF.`scriptTable` as plusFun\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as res from dataTable as output;\\n```\\n\\n\\n### Python UDAF Example\\n\\n```sql\\nset plusFun='''\\nfrom org.apache.spark.sql.expressions import MutableAggregationBuffer, UserDefinedAggregateFunction\\nfrom org.apache.spark.sql.types import DataTypes,StructType\\nfrom org.apache.spark.sql import Row\\nimport java.lang.Long as l\\nimport java.lang.Integer as i\\n\\nclass SumAggregation:\\n\\n    def inputSchema(self):\\n        return StructType().add(\\\"a\\\", DataTypes.LongType)\\n\\n    def bufferSchema(self):\\n        return StructType().add(\\\"total\\\", DataTypes.LongType)\\n\\n    def dataType(self):\\n        return DataTypes.LongType\\n\\n    def deterministic(self):\\n        return True\\n\\n    def initialize(self,buffer):\\n        return buffer.update(i(0), l(0))\\n\\n    def update(self,buffer, input):\\n        sum = buffer.getLong(i(0))\\n        newitem = input.getLong(i(0))\\n        buffer.update(i(0), l(sum + newitem))\\n\\n    def merge(self,buffer1, buffer2):\\n        buffer1.update(i(0), l(buffer1.getLong(i(0)) + buffer2.getLong(i(0))))\\n\\n    def evaluate(self,buffer):\\n        return buffer.getLong(i(0))\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\nand lang=\\\"python\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n### Scala UDAF Script Example\\n\\n```sql\\nset plusFun='''\\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\\nimport org.apache.spark.sql.types._\\nimport org.apache.spark.sql.Row\\nclass SumAggregation extends UserDefinedAggregateFunction with Serializable{\\n    def inputSchema: StructType = new StructType().add(\\\"a\\\", LongType)\\n    def bufferSchema: StructType =  new StructType().add(\\\"total\\\", LongType)\\n    def dataType: DataType = LongType\\n    def deterministic: Boolean = true\\n    def initialize(buffer: MutableAggregationBuffer): Unit = {\\n      buffer.update(0, 0l)\\n    }\\n    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\\n      val sum   = buffer.getLong(0)\\n      val newitem = input.getLong(0)\\n      buffer.update(0, sum + newitem)\\n    }\\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\\n      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))\\n    }\\n    def evaluate(buffer: Row): Any = {\\n      buffer.getLong(0)\\n    }\\n}\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n\\n### Some tricks\\n\\nYou can simplify the definition of UDF register like following:\\n\\n```sql\\nregister ScriptUDF.`` as count_board options lang=\\\"python\\\"\\n    and methodName=\\\"apply\\\"\\n    and dataType=\\\"map(string,integer)\\\"\\n    and code='''\\ndef apply(self, s):\\n    from collections import Counter\\n    return dict(Counter(s))\\n    '''\\n;\\n```\\n\\n\\nMulti methods defined onetime is also supported.\\n\\n```sql\\n\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\ndef hello(a:String)={\\n   s\\\"hello: ${a}\\\"\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as plusFun;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as plus, helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\nYou can also define this methods in a class:\\n\\n```sql\\n\\nset plusFun='''\\n\\nclass ScalaScript {\\n    def apply(a:Double,b:Double)={\\n       a + b\\n    }\\n\\n    def hello(a:String)={\\n       s\\\"hello: ${a}\\\"\\n    }\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\nand className=\\\"ScalaScript\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\n\\n\\n    \",\"docType\":\"md\"},{\"name\":\"ScalerInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Feature_scaling\\\">Feature scaling</a>\\n\\n Feature scaling is a method used to normalize the range of independent variables or features of data.\\n In data processing, it is also known as data normalization and\\n is generally performed during the data preprocessing step.\\n\\n Use \\\"load modelParams.`ScalerInPlace` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\\n    \",\"docType\":\"html\"},{\"name\":\"SchedulerCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"SchemaCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ScriptUDF\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n## Script support\\n\\nScript e.g. Python,Scala nested in MLSQL provides more fine-grained control when doing some ETL tasks, as it allows you\\neasily create SQL function with more powerful language which can do complex logical task.\\n\\nCause the tedious of java's grammar, we will not support java script.\\n\\nBefore use ScriptUDF module, you can use\\n\\n```\\nload modelParams.`ScriptUDF` as output;\\n```\\n\\nto check how to configure this module.\\n\\n### Python UDF Script Example\\n\\n```sql\\n-- using set statement to hold your python script\\n-- Notice that the first parameter of function you defined should be self.\\nset echoFun='''\\n\\ndef apply(self,m):\\n    return m\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be processed more conveniently.\\nload script.`echoFun` as scriptTable;\\n\\n-- register `apply` as UDF named `echoFun`\\nregister ScriptUDF.`scriptTable` as echoFun options\\n-- specify which script you choose\\nand lang=\\\"python\\\"\\n-- As we know python is not strongly typed language, so\\n-- we should manually spcify the return type.\\n-- map(string,string) means a map with key is string type,value also is string type.\\n-- array(string) means a array with string type element.\\n-- nested is support e.g. array(array(map(string,array(string))))\\nand dataType=\\\"map(string,string)\\\"\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect echoFun(map('a','b')) as res from dataTable as output;\\n```\\n\\n### Scala UDF Script Example\\n\\n```sql\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be process more convenient.\\nload script.`plusFun` as scriptTable;\\n\\n-- register `apply` as UDF named `plusFun`\\nregister ScriptUDF.`scriptTable` as plusFun\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as res from dataTable as output;\\n```\\n\\n\\n### Python UDAF Example\\n\\n```sql\\nset plusFun='''\\nfrom org.apache.spark.sql.expressions import MutableAggregationBuffer, UserDefinedAggregateFunction\\nfrom org.apache.spark.sql.types import DataTypes,StructType\\nfrom org.apache.spark.sql import Row\\nimport java.lang.Long as l\\nimport java.lang.Integer as i\\n\\nclass SumAggregation:\\n\\n    def inputSchema(self):\\n        return StructType().add(\\\"a\\\", DataTypes.LongType)\\n\\n    def bufferSchema(self):\\n        return StructType().add(\\\"total\\\", DataTypes.LongType)\\n\\n    def dataType(self):\\n        return DataTypes.LongType\\n\\n    def deterministic(self):\\n        return True\\n\\n    def initialize(self,buffer):\\n        return buffer.update(i(0), l(0))\\n\\n    def update(self,buffer, input):\\n        sum = buffer.getLong(i(0))\\n        newitem = input.getLong(i(0))\\n        buffer.update(i(0), l(sum + newitem))\\n\\n    def merge(self,buffer1, buffer2):\\n        buffer1.update(i(0), l(buffer1.getLong(i(0)) + buffer2.getLong(i(0))))\\n\\n    def evaluate(self,buffer):\\n        return buffer.getLong(i(0))\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\nand lang=\\\"python\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n### Scala UDAF Script Example\\n\\n```sql\\nset plusFun='''\\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\\nimport org.apache.spark.sql.types._\\nimport org.apache.spark.sql.Row\\nclass SumAggregation extends UserDefinedAggregateFunction with Serializable{\\n    def inputSchema: StructType = new StructType().add(\\\"a\\\", LongType)\\n    def bufferSchema: StructType =  new StructType().add(\\\"total\\\", LongType)\\n    def dataType: DataType = LongType\\n    def deterministic: Boolean = true\\n    def initialize(buffer: MutableAggregationBuffer): Unit = {\\n      buffer.update(0, 0l)\\n    }\\n    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\\n      val sum   = buffer.getLong(0)\\n      val newitem = input.getLong(0)\\n      buffer.update(0, sum + newitem)\\n    }\\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\\n      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))\\n    }\\n    def evaluate(buffer: Row): Any = {\\n      buffer.getLong(0)\\n    }\\n}\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n\\n### Some tricks\\n\\nYou can simplify the definition of UDF register like following:\\n\\n```sql\\nregister ScriptUDF.`` as count_board options lang=\\\"python\\\"\\n    and methodName=\\\"apply\\\"\\n    and dataType=\\\"map(string,integer)\\\"\\n    and code='''\\ndef apply(self, s):\\n    from collections import Counter\\n    return dict(Counter(s))\\n    '''\\n;\\n```\\n\\n\\nMulti methods defined onetime is also supported.\\n\\n```sql\\n\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\ndef hello(a:String)={\\n   s\\\"hello: ${a}\\\"\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as plusFun;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as plus, helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\nYou can also define this methods in a class:\\n\\n```sql\\n\\nset plusFun='''\\n\\nclass ScalaScript {\\n    def apply(a:Double,b:Double)={\\n       a + b\\n    }\\n\\n    def hello(a:String)={\\n       s\\\"hello: ${a}\\\"\\n    }\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\nand className=\\\"ScalaScript\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\n\\n\\n    \",\"docType\":\"md\"},{\"name\":\"SendMessage\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n SendMessage provides the ability to send messages externally. Currently, it only supports sending messages by mail, and supports single mail and batch mailing. The mail service provides two ways, one is to configure the email account of the mail sender through MLSQL and directly connect to the SMTP service to send mail, and the other is to connect to the local sendmail service to send mail.\\n\\nscenes to be used:\\n1. After the data is calculated and processed, a download link is generated and emailed to relevant personnel\\n2. When the amount of data is small, the data processing result can be sent directly\\n\\n Use \\\"load modelParams.`SendMessage` as output;\\\"\\n to check the available parameters;\\n\\n Use \\\"load modelExample.`SendMessage` as output;\\\"\\n get example.\\n    \",\"docType\":\"html\"},{\"name\":\"SendMultiMails\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n SendMultiMails provides the ability to send multiple mails externally. The mail service provides two ways, one is to configure the email account of the mail sender through MLSQL and directly connect to the SMTP service to send mail, and the other is to connect to the local sendmail service to send mail.\\n\\nscenes to be used:\\n1. After the data is calculated and processed, send the result to multiple users\\n2. When the amount of data is small, the data processing result can be sent directly\\n\\nconfiguration:\\n1. Set parameters into a table and specify it through the 'paramTab' parameter. Make Sure column names equals to parameter names.\\n2. Set parameters after `where` statement as global config. And parameter with the same name will be overwritten.\\n\\n Use \\\"load modelParams.`SendMultiMails` as output;\\\"\\n to check the available parameters;\\n\\n Use \\\"load modelExample.`SendMultiMails` as output;\\\"\\n get example.\\n    \",\"docType\":\"html\"},{\"name\":\"ShellExecute\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ShowCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ShowFunctionsExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ShowTableExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"ShowTablesExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"StandardScaler\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"StreamJobs\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"StringIndex\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"SyntaxAnalyzeExt\",\"algType\":\"feature engineer\",\"sparkCompatibility\":\"\",\"doc\":\"\\n SyntaxAnalyzeExt is used to parse the SQL grammar in the statement, please\\n check the codeExample to see how to use it.\\n\\n Use \\\"load modelParams.`SyntaxAnalyzeExt` as output;\\\"\\n to check the available parameters;\\n\\n Use \\\"load modelExample.`SyntaxAnalyzeExt` as output;\\\"\\n get example.\\n    \",\"docType\":\"html\"},{\"name\":\"TableRepartition\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\nChange the number of partitions. For example, before saving the file, or if we use python, we want python\\nworkers to run in parallel as much as possible. At this time, we need use TableRepartition.\\n    \",\"docType\":\"md\"},{\"name\":\"TableToMap\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"TfIdfInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n<a href=\\\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\\\"> tf–idf </a>\\n\\nIn information retrieval, tf–idf, TF*IDF, or TFIDF, short for term frequency–inverse document frequency,\\nis a numerical statistic that is intended to reflect how important a word is to a document in a collection\\nor corpus. It is often used as a weighting factor in searches of information retrieval, text mining,\\nand user modeling. The tf–idf value increases proportionally to the number of times a word appears\\nin the document and is offset by the number of documents in the corpus that contain the word, which\\nhelps to adjust for the fact that some words appear more frequently in general. tf–idf is one of\\nthe most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of\\ntext-based recommender systems in digital libraries use tf–idf.\\n\\n\\n Use \\\"load modelParams.`TfIdfInPlace` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\",\"docType\":\"html\"},{\"name\":\"ThenCommand\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Throw\",\"algType\":\"feature engineer\",\"sparkCompatibility\":\"\",\"doc\":\"\\n\\n This ET is used to stop the execute of the script.\\n    \",\"docType\":\"html\"},{\"name\":\"TokenAnalysis\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"TokenExtract\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"TreeBuildExt\",\"algType\":\"feature engineer\",\"sparkCompatibility\":\"\",\"doc\":\"\\n  TreeBuildExt used to build a tree when you have father - child relationship in some table,\\n  please check the codeExample to see how to use it.\\n    \",\"docType\":\"html\"},{\"name\":\"UploadFileToServerExt\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"VecMapInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"WaterMarkInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Word2ArrayInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"text\"},{\"name\":\"Word2VecInPlace\",\"algType\":\"undefined\",\"sparkCompatibility\":\"\",\"doc\":\"\\n <a href=\\\"https://en.wikipedia.org/wiki/Word2vec\\\">Word2vec</a>\\n\\n Word2vec is a technique for natural language processing published in 2013.\\n The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\\n Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.\\n As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector.\\n The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between\\n the vectors) indicates the level of semantic similarity between the words represented by those vectors.\\n\\n Use \\\"load modelParams.`Word2VecInPlace-` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\\n    \",\"docType\":\"html\"},{\"name\":\"XGBoostExt\",\"algType\":\"algorithm\",\"sparkCompatibility\":\"\",\"doc\":\"\",\"docType\":\"html\"}]";

    protected static final String DEFAULT_ADMIN_USER = "admin";

    protected static final String MOCK_NOTEBOOK = "test_import_notebook.bznb";

    @ClassRule
    public static final Network byzerNetwork = Network.newNetwork();

    protected static MockServerClient client = null;

    public static final DockerImageName MOCKSERVER_IMAGE = DockerImageName.parse("jamesdbloom/mockserver:mockserver-5.5.4");

    public static final MockServerContainer mockServer = new MockServerContainer(MOCKSERVER_IMAGE)
            .withEnv("testcontainers.reuse.enable", "true")
            .withNetworkAliases("notebook-network")
            .withStartupAttempts(3)
            .withReuse(true);

    public static final MySQLContainer mockDatabase = (MySQLContainer) new MySQLContainer("mysql:5.7")
            .withDatabaseName("notebook")
            .withUsername("root")
            .withPassword("root");

    /*
      -DNOTEBOOK_HOME=/opt/projects/kyligence/byzer-notebook
      -Dspring.config.name=application,notebook
      -Djava.io.tmpdir=./tmp
      */
    static {
        mockServer.withCreateContainerCmdModifier(cmd -> cmd.withName("mockserver_mockserver-5.5.4"));
        String basePath = null;
        String testClassPath = null;
        try {
            testClassPath = NotebookLauncherBaseTest.class.getResource("/").getPath();
            File directory = new File(testClassPath + "../../");
            basePath = directory.getCanonicalPath() + "/";
        } catch (Exception e) {
            log.error("Can not get lib path, try to use absolute path. e:" + e.getMessage());
        }
        if (basePath == null) {
            basePath = "./";
        }
        if (testClassPath == null) {
            testClassPath = "./target/test-classes";
        }
        System.setProperty("NOTEBOOK_HOME", basePath);
        log.info("Current NOTEBOOK_HOME is {}", basePath);
        System.setProperty("java.io.tmpdir", "./tmp");
        System.setProperty("PROPERTIES_PATH", testClassPath);

        mockServer.start();
        mockDatabase.start();
        client = new MockServerClient(mockServer.getHost(), mockServer.getServerPort());
        assertTrue("Mockserver running", client.isRunning());

        // used by !show et;
        client.when(request().withPath("/health/liveness").withMethod("GET"))
                .respond(response().withStatusCode(200));
        client.when(request().withPath("/health/readiness").withMethod("GET"))
                .respond(response().withStatusCode(200));
        client.when(request().withPath("/run/script").withMethod("POST")
        ).respond(response().withBody(etResponse));
        System.setProperty("spring.datasource.url", mockDatabase.getJdbcUrl());
        System.setProperty("spring.datasource.driver-class-name", mockDatabase.getDriverClassName());
        System.setProperty("spring.datasource.username", mockDatabase.getUsername());
        System.setProperty("spring.datasource.password", mockDatabase.getPassword());
        System.setProperty("notebook.database.ip", mockDatabase.getHost());
        System.setProperty("notebook.database.port", mockDatabase.getFirstMappedPort().toString());
        System.setProperty("notebook.mlsql.engine-url", String.format("http://%s:%s", mockServer.getHost(), mockServer.getServerPort()));
    }

    @Autowired
    protected WebApplicationContext context;

    @LocalServerPort
    private int port;

    private URL base;

    @Autowired
    protected TestRestTemplate restTemplate;

    @Autowired
    protected MockMvc mvc;

    protected String defaultMockNotebookId;

    public ResultActions importNotebook(String notebookName) throws Exception {
        MockMultipartFile file = new MockMultipartFile("file", notebookName, "application/json", getInputStream(notebookName));
        return this.mvc.perform(
                MockMvcRequestBuilders.multipart("/api/file/import")
                        .file(file)
                        .contentType(DEFAULT_CONTENT_TYPE)
                        .header(DEFAULT_AUTH_HEADER, DEFAULT_AUTH_TOKEN)
        );
    }


    @Before
    public void setUp() throws Exception {
        String url = String.format("http://localhost:%d/", port);
        System.out.printf("port is : [%d]%n", port);
        this.base = new URL(url);
    }

    @Override
    public String getCollectionName() {return "base";}

    @PostConstruct
    protected void mock() throws Exception {
        ImportResponseDTO dto = JacksonUtils.readJson(
                this.importNotebook(MOCK_NOTEBOOK)
                .andReturn()
                .getResponse()
                .getContentAsString(),
                ImportResponseDTO.class);
        this.defaultMockNotebookId = Objects.requireNonNull(dto).getData().get(0).getId();
    }


    /**
     * Sent to /. Make sure the web application is started. Should be package the static resource
     */
    @Test
    public void testHomePage() {
        ResponseEntity<String> response = this.restTemplate.getForEntity(
                this.base.toString() + "/", String.class, "");
        System.out.printf("testHomePage result is ：%s%n", response.getBody());
    }

    public void clean(String path, String method, MockServerClient client) {
        HttpRequest post = request()
                .withPath(path);
        if (method != null) {
            post = post.withMethod("POST");
        }

        if (Objects.nonNull(client) && client.isRunning()) {
            client.clear(post,
                    ClearType.LOG
            );
        }
    }
}

/**
 * -Dspring.config.location=classpath:/,file:./conf/
 */
@TestPropertySource(properties = {"spring.config.location=classpath:/,file:./"})
@TestConfiguration
class RelationalDbDaoConfiguration {
}